{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlO+xoUC5uvpqviBoAwTAD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthog/MLHW/blob/main/HW7_MattHogan_801109363.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBWcZXss9BUG",
        "outputId": "7475c915-be80-49cd-e68a-3f1949107ed0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Training Baseline CNN\n",
            "Epoch 1/200, Loss: 1.3267\n",
            "Epoch 2/200, Loss: 0.9497\n",
            "Epoch 3/200, Loss: 0.7962\n",
            "Epoch 4/200, Loss: 0.6877\n",
            "Epoch 5/200, Loss: 0.5896\n",
            "Epoch 6/200, Loss: 0.4952\n",
            "Epoch 7/200, Loss: 0.4105\n",
            "Epoch 8/200, Loss: 0.3297\n",
            "Epoch 9/200, Loss: 0.2626\n",
            "Epoch 10/200, Loss: 0.2045\n",
            "Epoch 11/200, Loss: 0.1576\n",
            "Epoch 12/200, Loss: 0.1255\n",
            "Epoch 13/200, Loss: 0.1141\n",
            "Epoch 14/200, Loss: 0.0931\n",
            "Epoch 15/200, Loss: 0.0779\n",
            "Epoch 16/200, Loss: 0.0767\n",
            "Epoch 17/200, Loss: 0.0611\n",
            "Epoch 18/200, Loss: 0.0608\n",
            "Epoch 19/200, Loss: 0.0756\n",
            "Epoch 20/200, Loss: 0.0554\n",
            "Epoch 21/200, Loss: 0.0499\n",
            "Epoch 22/200, Loss: 0.0597\n",
            "Epoch 23/200, Loss: 0.0534\n",
            "Epoch 24/200, Loss: 0.0532\n",
            "Epoch 25/200, Loss: 0.0540\n",
            "Epoch 26/200, Loss: 0.0407\n",
            "Epoch 27/200, Loss: 0.0364\n",
            "Epoch 28/200, Loss: 0.0527\n",
            "Epoch 29/200, Loss: 0.0453\n",
            "Epoch 30/200, Loss: 0.0406\n",
            "Epoch 31/200, Loss: 0.0322\n",
            "Epoch 32/200, Loss: 0.0476\n",
            "Epoch 33/200, Loss: 0.0434\n",
            "Epoch 34/200, Loss: 0.0383\n",
            "Epoch 35/200, Loss: 0.0401\n",
            "Epoch 36/200, Loss: 0.0296\n",
            "Epoch 37/200, Loss: 0.0356\n",
            "Epoch 38/200, Loss: 0.0378\n",
            "Epoch 39/200, Loss: 0.0407\n",
            "Epoch 40/200, Loss: 0.0370\n",
            "Epoch 41/200, Loss: 0.0282\n",
            "Epoch 42/200, Loss: 0.0366\n",
            "Epoch 43/200, Loss: 0.0433\n",
            "Epoch 44/200, Loss: 0.0305\n",
            "Epoch 45/200, Loss: 0.0343\n",
            "Epoch 46/200, Loss: 0.0315\n",
            "Epoch 47/200, Loss: 0.0360\n",
            "Epoch 48/200, Loss: 0.0367\n",
            "Epoch 49/200, Loss: 0.0306\n",
            "Epoch 50/200, Loss: 0.0310\n",
            "Epoch 51/200, Loss: 0.0327\n",
            "Epoch 52/200, Loss: 0.0270\n",
            "Epoch 53/200, Loss: 0.0391\n",
            "Epoch 54/200, Loss: 0.0333\n",
            "Epoch 55/200, Loss: 0.0222\n",
            "Epoch 56/200, Loss: 0.0405\n",
            "Epoch 57/200, Loss: 0.0273\n",
            "Epoch 58/200, Loss: 0.0300\n",
            "Epoch 59/200, Loss: 0.0343\n",
            "Epoch 60/200, Loss: 0.0327\n",
            "Epoch 61/200, Loss: 0.0263\n",
            "Epoch 62/200, Loss: 0.0315\n",
            "Epoch 63/200, Loss: 0.0277\n",
            "Epoch 64/200, Loss: 0.0259\n",
            "Epoch 65/200, Loss: 0.0264\n",
            "Epoch 66/200, Loss: 0.0371\n",
            "Epoch 67/200, Loss: 0.0275\n",
            "Epoch 68/200, Loss: 0.0211\n",
            "Epoch 69/200, Loss: 0.0301\n",
            "Epoch 70/200, Loss: 0.0224\n",
            "Epoch 71/200, Loss: 0.0387\n",
            "Epoch 72/200, Loss: 0.0176\n",
            "Epoch 73/200, Loss: 0.0270\n",
            "Epoch 74/200, Loss: 0.0239\n",
            "Epoch 75/200, Loss: 0.0284\n",
            "Epoch 76/200, Loss: 0.0231\n",
            "Epoch 77/200, Loss: 0.0236\n",
            "Epoch 78/200, Loss: 0.0313\n",
            "Epoch 79/200, Loss: 0.0290\n",
            "Epoch 80/200, Loss: 0.0269\n",
            "Epoch 81/200, Loss: 0.0249\n",
            "Epoch 82/200, Loss: 0.0261\n",
            "Epoch 83/200, Loss: 0.0215\n",
            "Epoch 84/200, Loss: 0.0308\n",
            "Epoch 85/200, Loss: 0.0310\n",
            "Epoch 86/200, Loss: 0.0187\n",
            "Epoch 87/200, Loss: 0.0266\n",
            "Epoch 88/200, Loss: 0.0267\n",
            "Epoch 89/200, Loss: 0.0223\n",
            "Epoch 90/200, Loss: 0.0255\n",
            "Epoch 91/200, Loss: 0.0205\n",
            "Epoch 92/200, Loss: 0.0284\n",
            "Epoch 93/200, Loss: 0.0185\n",
            "Epoch 94/200, Loss: 0.0276\n",
            "Epoch 95/200, Loss: 0.0242\n",
            "Epoch 96/200, Loss: 0.0320\n",
            "Epoch 97/200, Loss: 0.0281\n",
            "Epoch 98/200, Loss: 0.0140\n",
            "Epoch 99/200, Loss: 0.0312\n",
            "Epoch 100/200, Loss: 0.0224\n",
            "Epoch 101/200, Loss: 0.0221\n",
            "Epoch 102/200, Loss: 0.0327\n",
            "Epoch 103/200, Loss: 0.0154\n",
            "Epoch 104/200, Loss: 0.0326\n",
            "Epoch 105/200, Loss: 0.0209\n",
            "Epoch 106/200, Loss: 0.0225\n",
            "Epoch 107/200, Loss: 0.0235\n",
            "Epoch 108/200, Loss: 0.0203\n",
            "Epoch 109/200, Loss: 0.0260\n",
            "Epoch 110/200, Loss: 0.0219\n",
            "Epoch 111/200, Loss: 0.0217\n",
            "Epoch 112/200, Loss: 0.0224\n",
            "Epoch 113/200, Loss: 0.0286\n",
            "Epoch 114/200, Loss: 0.0185\n",
            "Epoch 115/200, Loss: 0.0216\n",
            "Epoch 116/200, Loss: 0.0349\n",
            "Epoch 117/200, Loss: 0.0223\n",
            "Epoch 118/200, Loss: 0.0256\n",
            "Epoch 119/200, Loss: 0.0232\n",
            "Epoch 120/200, Loss: 0.0232\n",
            "Epoch 121/200, Loss: 0.0212\n",
            "Epoch 122/200, Loss: 0.0182\n",
            "Epoch 123/200, Loss: 0.0254\n",
            "Epoch 124/200, Loss: 0.0226\n",
            "Epoch 125/200, Loss: 0.0135\n",
            "Epoch 126/200, Loss: 0.0327\n",
            "Epoch 127/200, Loss: 0.0262\n",
            "Epoch 128/200, Loss: 0.0163\n",
            "Epoch 129/200, Loss: 0.0248\n",
            "Epoch 130/200, Loss: 0.0268\n",
            "Epoch 131/200, Loss: 0.0154\n",
            "Epoch 132/200, Loss: 0.0230\n",
            "Epoch 133/200, Loss: 0.0196\n",
            "Epoch 134/200, Loss: 0.0246\n",
            "Epoch 135/200, Loss: 0.0175\n",
            "Epoch 136/200, Loss: 0.0260\n",
            "Epoch 137/200, Loss: 0.0250\n",
            "Epoch 138/200, Loss: 0.0227\n",
            "Epoch 139/200, Loss: 0.0288\n",
            "Epoch 140/200, Loss: 0.0204\n",
            "Epoch 141/200, Loss: 0.0221\n",
            "Epoch 142/200, Loss: 0.0254\n",
            "Epoch 143/200, Loss: 0.0160\n",
            "Epoch 144/200, Loss: 0.0241\n",
            "Epoch 145/200, Loss: 0.0250\n",
            "Epoch 146/200, Loss: 0.0246\n",
            "Epoch 147/200, Loss: 0.0220\n",
            "Epoch 148/200, Loss: 0.0187\n",
            "Epoch 149/200, Loss: 0.0214\n",
            "Epoch 150/200, Loss: 0.0264\n",
            "Epoch 151/200, Loss: 0.0220\n",
            "Epoch 152/200, Loss: 0.0202\n",
            "Epoch 153/200, Loss: 0.0232\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# CIFAR-10 data preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Baseline CNN\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Extended CNN\n",
        "class ExtendedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ExtendedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Training and evaluation\n",
        "def train_and_evaluate(model, epochs=200):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Training Time: {training_time:.2f}s, Test Accuracy: {accuracy:.2f}%\")\n",
        "    return training_time, running_loss / len(train_loader), accuracy\n",
        "\n",
        "# Baseline CNN\n",
        "print(\"Training Baseline CNN\")\n",
        "baseline_model = BaselineCNN()\n",
        "baseline_results = train_and_evaluate(baseline_model)\n",
        "\n",
        "# Extended CNN\n",
        "print(\"\\nTraining Extended CNN\")\n",
        "extended_model = ExtendedCNN()\n",
        "extended_results = train_and_evaluate(extended_model)\n",
        "\n",
        "# Report results\n",
        "print(\"\\nResults Comparison:\")\n",
        "print(f\"Baseline CNN - Time: {baseline_results[0]:.2f}s, Loss: {baseline_results[1]:.4f}, Accuracy: {baseline_results[2]:.2f}%\")\n",
        "print(f\"Extended CNN - Time: {extended_results[0]:.2f}s, Loss: {extended_results[1]:.4f}, Accuracy: {extended_results[2]:.2f}%\")\n"
      ]
    }
  ]
}